[{"authors":null,"categories":null,"content":"","date":1558031400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558031400,"objectID":"d77302ef075f8fcb030df7a3e021eb89","permalink":"sriharshavenugopal.github.io/talk/spectral/","publishdate":"2019-05-17T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/spectral/","section":"talk","summary":"This is introductory talk to Spectral grpah theory. The talk starts with how to view Matrices as Graphs and end with why do we need spectral graph theory and its applications eg., spectral clustering. There are two python scripts which further helps in spectral graph theory","tags":["Deep Learning","Convolutional Neural Networks","Semantic segmentation"],"title":"Spectral Graph Theory","type":"talk"},{"authors":null,"categories":[],"content":"","date":1542910411,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542910411,"objectID":"f910178b8f059acaff5fdedab2b0ab75","permalink":"sriharshavenugopal.github.io/project/cloning/","publishdate":"2018-11-22T23:43:31+05:30","relpermalink":"sriharshavenugopal.github.io/project/cloning/","section":"project","summary":"","tags":["Computer Vision","Autonomous Navigation","Deep Learning"],"title":"Behavioral Cloning","type":"project"},{"authors":null,"categories":[],"content":"","date":1542910411,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542910411,"objectID":"d2aed5c3f060228373887554f6cb2a0f","permalink":"sriharshavenugopal.github.io/project/lane/","publishdate":"2018-11-22T23:43:31+05:30","relpermalink":"sriharshavenugopal.github.io/project/lane/","section":"project","summary":"","tags":["Computer Vision","Autonomous Navigation","Deep Learning"],"title":"Lane Line Detection","type":"project"},{"authors":null,"categories":[],"content":"","date":1542910411,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542910411,"objectID":"06e01055f25864487067a689f891da14","permalink":"sriharshavenugopal.github.io/project/object/","publishdate":"2018-11-22T23:43:31+05:30","relpermalink":"sriharshavenugopal.github.io/project/object/","section":"project","summary":"","tags":["Computer Vision"],"title":"POD: Discovering Primary Objects in Videos Based on Evolutionary Refinement of Object Recurrence, Background, and Primary Object Models","type":"project"},{"authors":null,"categories":[],"content":"","date":1542910411,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542910411,"objectID":"060c916ac508dd34fd140ae0326b0fdb","permalink":"sriharshavenugopal.github.io/project/traffic/","publishdate":"2018-11-22T23:43:31+05:30","relpermalink":"sriharshavenugopal.github.io/project/traffic/","section":"project","summary":"","tags":["Computer Vision","Autonomous Navigation","Deep Learning"],"title":"Traffic Sign Recognition","type":"project"},{"authors":null,"categories":[],"content":"","date":1542910411,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542910411,"objectID":"e3d3eb80b346d58cfb398ea212aae5f5","permalink":"sriharshavenugopal.github.io/project/object-segmentation-detection/index/","publishdate":"2018-11-22T23:43:31+05:30","relpermalink":"sriharshavenugopal.github.io/project/object-segmentation-detection/index/","section":"project","summary":"","tags":["Computer Vision"],"title":"VIDEO OBJECT SEGMENTATION AGGREGATION","type":"project"},{"authors":null,"categories":[],"content":"","date":1542910411,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542910411,"objectID":"9a4aeb92dac7b8c02278a4c0685bc64a","permalink":"sriharshavenugopal.github.io/project/vehicle/","publishdate":"2018-11-22T23:43:31+05:30","relpermalink":"sriharshavenugopal.github.io/project/vehicle/","section":"project","summary":"","tags":["Computer Vision","Autonomous Navigation","Deep Learning"],"title":"Vehicle Detection","type":"project"},{"authors":["Evan Shelhamer","Kate Rakelly","Judy Hoffman","Trevor Darrell"],"categories":null,"content":"","date":1535049000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535049000,"objectID":"0263cd3a080a4e6de81255d79780aaa7","permalink":"sriharshavenugopal.github.io/talk/clockwork/","publishdate":"2018-08-24T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/clockwork/","section":"talk","summary":"Recent years have seen tremendous progress in still-image segmentation; however the naıve application of these state-of-the-art algorithms to every video frame requires considerable computation and ignores the temporal continuity inherent in video. We propose a video recognition framework that relies on two key observations: 1) while pixels may change rapidly from frame to frame, the semantic content of a scene evolves more slowly, and 2) execution can be  viewed as an aspect of architecture, yielding purpose-fit computation schedules for networks. We define a novel family of “clockwork” convnets driven by fixed or adaptive clock signals that schedule the processing of different layers at different update rates according to their semantic stability. We design a pipeline schedule to reduce latency for real-time recognition and a fixed-rate schedule to reduce overall computation. Finally, we extend clockwork scheduling to adaptive video processing by incorporating data-driven clocks that can be tuned on unlabeled video. The accuracy and efficiency of clockwork convnets are evaluated on the Youtube-Objects, NYUD, and Cityscapes video datasets.","tags":["Deep Learning","Convolutional Neural Networks","Semantic segmentation","Video Compression"],"title":"Clockwork FCN","type":"talk"},{"authors":["Nikitha Vallurapalli\u003csup\u003e1\u003c/sup\u003e","**Sriharsha Annamaneni**\u003csup\u003e1\u003c/sup\u003e","Girish Varma\u003csup\u003e1\u003c/sup\u003e","C V Jawahar\u003csup\u003e1\u003c/sup\u003e","Manu Mathew","Soyeb Nagori"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"3094628f806da2c79d1160da74371d13","permalink":"sriharshavenugopal.github.io/publication/efficient-semantic-segmentation-using-gradual-grouping/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"sriharshavenugopal.github.io/publication/efficient-semantic-segmentation-using-gradual-grouping/","section":"publication","summary":"Deep CNNs for semantic segmentation have high memory and run time requirements. Various approaches have been proposed to make CNNs efficient like grouped, shuffled, depth-wise separable convolutions. We study the effectiveness of these techniques on a real-time semantic segmentation architecture like ERFNet for improving runtime by over 5X. We apply these techniques to CNN layers partially or fully and evaluate the testing accuracies on Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs, giving accuracy scores for models that can run under specified runtime budgets. *We further propose a novel training procedure which starts out with a dense convolution but gradually evolves towards a grouped convolution. We show that our proposed training method and efficient architecture design can improve accuracies by over 8% with depthwise separable convolutions applied on the encoder of ERFNet and attaching a light weight decoder. This results in a model which has a 5X improvement in FLOPs while only suffering a 4% degradatioon in accuracy with respect to ERFNet*","tags":null,"title":"Efficient Semantic Segmentation using Gradual Grouping (oral) (Best paper Runner up Award)","type":"publication"},{"authors":["Chuyuan Li","Heerad Farkhoor","Rosanne Liu","Jason Yosinski"],"categories":null,"content":"","date":1526495400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526495400,"objectID":"0a70e8f5f30a89b3378b7837afca1872","permalink":"sriharshavenugopal.github.io/talk/intrinsic/","publishdate":"2018-05-17T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/intrinsic/","section":"talk","summary":"Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.","tags":["Deep Learning","training"],"title":"Measuring the Intrinsic Dimension of Objective Landscapes","type":"talk"},{"authors":["Mengye Ren","Wenyuan Zeng","Bin Yang","Raquel Urtasun"],"categories":null,"content":"","date":1525977000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525977000,"objectID":"f439309534f69e2261073387f3f54bff","permalink":"sriharshavenugopal.github.io/talk/reweight/","publishdate":"2018-05-11T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/reweight/","section":"talk","summary":"Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available","tags":["Deep Learning","Supervised Learning","Overfitting"],"title":"Learning to Reweight Examples for Robust Deep Learning","type":"talk"},{"authors":null,"categories":null,"content":"","date":1524681000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524681000,"objectID":"fc93779457c9fac86bfa62048631e65b","permalink":"sriharshavenugopal.github.io/talk/semantic/","publishdate":"2018-04-26T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/semantic/","section":"talk","summary":"Overview of Semantic Segmentation Architectures.","tags":["Deep Learning","Convolutional Neural Networks","Semantic segmentation"],"title":"Overview of Semantic Segmentation","type":"talk"},{"authors":["Z. Wu","T. Nagarajan","S. Rennie","L. Davis","K. Grauman","R. Feris"],"categories":null,"content":"","date":1524162600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524162600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"sriharshavenugopal.github.io/talk/example/","publishdate":"2018-04-20T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/example/","section":"talk","summary":"Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20% on average, going as high as 36% for some images, while maintaining the same 76.4% top-1 accuracy on ImageNet.","tags":["Deep Learning","Convolutional Neural Networks","Policy Gradients"],"title":"Block Drop: Dynamic Inference Paths in Residual Networks","type":"talk"},{"authors":["Gao Huang","Zhuang Liu","Laurens van der Maaten","Kilian Q. Weinberger"],"categories":null,"content":"","date":1523644200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523644200,"objectID":"3afeaf10af81badfdf8b22aa8d2a09b6","permalink":"sriharshavenugopal.github.io/talk/dense/dense/","publishdate":"2018-04-14T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/dense/dense/","section":"talk","summary":"Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections—one between each layer and its subsequent layer—our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance.","tags":["Deep Learning","Convolutional Neural Networks","Semantic segmentation"],"title":"Densely Connected Neural Networks","type":"talk"},{"authors":["Nikitha Vallurapalli","Sriharsha Annamaneni","Girish Varma","C.V. Jawahar","Manu MAthew","Soyeb Nagori"],"categories":null,"content":"","date":1522693800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522693800,"objectID":"ad9b4eaa15926824ca6807255ae71739","permalink":"sriharshavenugopal.github.io/talk/symposium/","publishdate":"2018-04-03T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/talk/symposium/","section":"talk","summary":"Deep CNNs for semantic segmentation have high memory and run time requirements. Various approaches have been proposed to make CNNs efficient like grouped, shuffled, depth-wise separable convolutions. We study the effectiveness of these techniques on a real-time semantic segmentation architecture like ERFNet for improving runtime by iver 5X. We apply these techniques to CNN layers partially or fully and evaluate the testing accuracies on Cityscapes dataset. We obtain accuracy vs parameters/FLOPs trade offs, giving accuracy scores for models that can run under specified runtime budgets. *We further propose a novel training procedure which starts out with a dense convolution but gradually evolves towards a grouped convolution. We show that our proposed training method and efficient architecture design can improve accuracies by over 8% with depthwise separable convolutions applied on the encoder of ERFNet and attaching a light weight decoder. This results in a model which has a 5X improvement in FLOPs while only suffering a 4% degradatioon in accuracy with respect to ERFNet*","tags":["Deep Learning","Convolutional Neural Networks","Semantic segmentation"],"title":"Efficient Semantic Segmentation using Gradual Grouping","type":"talk"},{"authors":[],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461090600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515781800,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"sriharshavenugopal.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00+05:30","relpermalink":"sriharshavenugopal.github.io/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":["Pramath Keny"," Arya Menon","Madhura Rao","Urvang Gaitonde","Animesh Gupta","**Annamaneni Sriharsha** "],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"39a582fca3cebf583537421bb8177f48","permalink":"sriharshavenugopal.github.io/publication/antenna/","publishdate":"2013-07-01T00:00:00Z","relpermalink":"sriharshavenugopal.github.io/publication/antenna/","section":"publication","summary":"Nano Satellites generally weighing between 1 and 15 kg, serving the purpose of space research are popularly standardized as CubeSats. The CubeSat standard has made it really feasible and easy for students from various universities to develop their own Nano-satellites with their very own payloads. Antennas are critical components in the onboard communication system of satellites, the Nano Satellites usually communicate in the Amateur Frequency Bands; these bands exist from 144 MHz to 146 MHz in VHF and from 434 MHz to 438 MHz in the UHF range. Designing antennas at these frequencies typically ends up being larger in size than the actual CubeSat itself. Thus generally the antennas for a Nano Satellite are made flexible enough to be folded in order to comply with the CubeSat size standards. Once the satellite is ejected into the orbit from the deployer module, an automated signal is used to trigger a circuit that initiates a series of processes to deploy the folded antennas back into their original shape. This paper deals about the design and development of a highly efficient, smart and reliable control circuit prototype called the Antenna Deployment Circuit. This developed prototype is tested and the results are summarized in the paper.","tags":null,"title":"Development of Antenna Deployment Circuit for Nano-Satellites ","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a36058eafe01cd26b16c80f89806a069","permalink":"sriharshavenugopal.github.io/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"sriharshavenugopal.github.io/skills/","section":"","summary":"","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":" title = \u0026ldquo;Example Page\u0026rdquo;\ndate = 2018-09-09T00:00:00\nlastmod = 2018-09-09T00:00:00 draft = false # Is this a draft? true/false toc = true # Show table of contents? true/false type = \u0026ldquo;docs\u0026rdquo; # Do not modify.\nAdd menu entry to sidebar. linktitle = \u0026ldquo;Example Page\u0026rdquo; [menu.tutorial] parent = \u0026ldquo;Example Topic\u0026rdquo; weight = 1 #+++\nIn this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 ..\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"sriharshavenugopal.github.io/tutorial/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"sriharshavenugopal.github.io/tutorial/example/","section":"tutorial","summary":"title = \u0026ldquo;Example Page\u0026rdquo;\ndate = 2018-09-09T00:00:00\nlastmod = 2018-09-09T00:00:00 draft = false # Is this a draft? true/false toc = true # Show table of contents? true/false type = \u0026ldquo;docs\u0026rdquo; # Do not modify.\nAdd menu entry to sidebar. linktitle = \u0026ldquo;Example Page\u0026rdquo; [menu.tutorial] parent = \u0026ldquo;Example Topic\u0026rdquo; weight = 1 #+++\nIn this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;","tags":null,"title":"","type":"tutorial"}]